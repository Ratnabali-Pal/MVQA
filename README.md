![Examples](vqa_norsk.pdf)

### Publication
Pal, Ratnabali, Samarjit Kar, and Dilip K. Prasad. ``NorViVQA: Visual Question Answering 
for Visually Impaired in Norwegian Language" NAIS 2025: Symposium of the Norwegian AI 
Society, June 17--18, 2025, Tromsø, Norway.

[Link to The Paper](coming soon )

### Abstract
 Designing a visual question answering (VQA) system for low-resource languages is challenging. Yet, it has
 enormous potential for practical applications toward making AI-driven assistive technologies more inclusive
 and accessible. In this article, we introduce Norsk VQA for visually impaired (NorViVQA), a Norwegian VQA
 dataset derived from the VizWiz-VQA, which contains real-world, often low-quality images captured by blind
 users alongside questions. Using the NorT5-base model, fine-tuned for English–Norwegian translation, we
 translate questions and answers into Norwegian Bokmål while preserving the original challenges of VizWiz. We
 propose alight-weight VQAmoduleontopoftheContrastiveLanguage-ImagePre-training(CLIP)foranswertype
 prediction and answer prediction. We demonstrated the effect of different Norsk embedding methods and achieved
 67.09% answer type prediction accuracy and 34.86% answer accuracy. This research aims to bridge the research
 gap in VQA for visually impaired users in the Norwegian language. The low accuracy of NorViVQA opened up
 newchallenges for the research community. The code and the datasets will be available in http://www.github.com

### License

Copyright © 2025 Ratnabali Pal

The content of this repository is bound by the following licenses:

- The documents and data are licensed under the MIT license.
